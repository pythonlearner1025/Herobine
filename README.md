# ğŸ® JarvisVLA Server Setup

**Serve JarvisVLA (Minecraft AI) via vLLM with public ngrok access**

This repository provides a **one-shot installation script** to serve the [JarvisVLA](https://github.com/CraftJarvis/JarvisVLA) vision-language-action model using vLLM, with proper dependency resolution to avoid the common "dependency hell" issues.

## ğŸš€ Quick Start (3 Commands)

```bash
# 1. Install everything (15-20 minutes)
sudo ./install.sh

# 2. Verify installation
./verify_installation.sh

# 3. Start serving
./start_vllm_server.sh
```

That's it! The model will auto-download on first use and start serving on port 8000.

## ğŸ“‹ What This Does

âœ… Installs vLLM with proper PyTorch dependencies  
âœ… Installs JarvisVLA package (without dependency conflicts)  
âœ… Installs Minecraft simulation environment (minestudio)  
âœ… Configures OpenJDK 8 for Minecraft  
âœ… Sets up ngrok for public HTTPS tunneling  
âœ… Provides ready-to-use serving scripts  

## ğŸ¯ Use Cases

- **Run inference** on JarvisVLA for Minecraft tasks
- **Expose local GPU** as a cloud API via ngrok
- **Test vision-language models** without complex setup
- **Deploy on VM** without SSL certificate hassles

## ğŸ“š Documentation

- **[INSTALLATION_GUIDE.md](INSTALLATION_GUIDE.md)** - Comprehensive setup guide, troubleshooting
- **[CHANGES_SUMMARY.md](CHANGES_SUMMARY.md)** - Technical details on how we fixed dependency hell
- **[verify_installation.sh](verify_installation.sh)** - Quick health check script

## ğŸ”§ Key Features

### 1. Proper Dependency Resolution
Unlike naive installation approaches, this script:
- Installs vLLM first (with full dependency resolution)
- Lets modern compatible versions coexist
- Skips training-only dependencies for serving

### 2. Flexible Configuration
```bash
# Custom GPU
CUDA_VISIBLE_DEVICES=1 ./start_vllm_server.sh

# Custom port
PORT=8080 ./start_vllm_server.sh

# Static ngrok domain
export NGROK_DOMAIN=your-domain.ngrok-free.dev
./start_ngrok_tunnel.sh
```

### 3. OpenAI-Compatible API
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"
)

response = client.chat.completions.create(
    model="JarvisVLA-Qwen2-VL-7B",
    messages=[{"role": "user", "content": "Mine diamond ore"}]
)
```

## ğŸ› ï¸ System Requirements

- **OS:** Ubuntu 20.04+ / Debian 11+
- **GPU:** NVIDIA GPU with 16GB+ VRAM (for 7B model)
- **RAM:** 32GB+ recommended
- **CUDA:** 11.8 or 12.x
- **Disk:** 20GB+ free space

## ğŸ“– Usage Examples

### Start Server Locally
```bash
./start_vllm_server.sh
# Server at http://localhost:8000
# API docs at http://localhost:8000/docs
```

### Expose Publicly via ngrok
```bash
# Terminal 1
./start_vllm_server.sh

# Terminal 2 (after server starts)
./start_ngrok_tunnel.sh
# Copy the public URL (e.g., https://xyz.ngrok-free.dev)
```

### Test API
```bash
curl http://localhost:8000/health
curl http://localhost:8000/v1/models

# Or visit in browser:
# http://localhost:8000/docs
```

## ğŸ› Troubleshooting

### Installation fails with dependency conflicts
```bash
# Clean pip cache and retry
pip cache purge
sudo ./install.sh
```

### CUDA out of memory
```bash
# Reduce max sequence length
MAX_MODEL_LEN=4096 ./start_vllm_server.sh
```

### vLLM server won't start
```bash
# Check CUDA availability
python -c "import torch; print(torch.cuda.is_available())"

# Check vLLM
python -c "import vllm; print('OK')"
```

See [INSTALLATION_GUIDE.md](INSTALLATION_GUIDE.md) for detailed troubleshooting.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Public Internet                 â”‚
â”‚  https://your-domain.ngrok-free.dev     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ ngrok tunnel
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Your VM / Local Machine            â”‚
â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   vLLM Server (Port 8000)      â”‚     â”‚
â”‚  â”‚   - OpenAI-compatible API      â”‚     â”‚
â”‚  â”‚   - JarvisVLA-Qwen2-VL-7B      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   GPU (CUDA)                   â”‚     â”‚
â”‚  â”‚   - Model inference            â”‚     â”‚
â”‚  â”‚   - xformers optimization      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ What Gets Installed

### Core Components
- **vLLM 0.10.2** - High-performance inference server
- **PyTorch 2.8.0** - With CUDA support
- **transformers** - HuggingFace model loading
- **JarvisVLA** - Vision-language-action model

### Supporting Libraries
- **xformers** - Attention kernel optimizations
- **qwen-vl-utils** - Vision-language processing
- **minestudio** - Minecraft simulation (optional for serving)

### Tools
- **ngrok** - HTTPS tunneling
- **OpenJDK 8** - For Minecraft simulation

## ğŸ” Security Notes

- vLLM server has no authentication by default (local use)
- Use ngrok authentication for public endpoints
- Don't expose sensitive data through the API
- Consider firewall rules for production

## ğŸ¤ Contributing

Issues and PRs welcome! Key areas:
- Multi-GPU support scripts
- Docker containerization
- Better error handling
- Cloud deployment guides

## ğŸ“„ License

See individual component licenses:
- JarvisVLA: MIT License
- vLLM: Apache 2.0
- This setup: MIT License

## ğŸ™ Credits

- [JarvisVLA Team](https://github.com/CraftJarvis/JarvisVLA) - Original model
- [vLLM Team](https://github.com/vllm-project/vllm) - Inference engine
- [ngrok](https://ngrok.com/) - Tunneling service

## ğŸ“ Support

- **Installation issues**: Check [INSTALLATION_GUIDE.md](INSTALLATION_GUIDE.md)
- **Dependency conflicts**: See [CHANGES_SUMMARY.md](CHANGES_SUMMARY.md)
- **vLLM issues**: [vLLM Documentation](https://docs.vllm.ai/)
- **JarvisVLA issues**: [JarvisVLA GitHub](https://github.com/CraftJarvis/JarvisVLA)

---

**Ready to serve JarvisVLA in under 20 minutes!** ğŸš€

Run `./install.sh` to get started.